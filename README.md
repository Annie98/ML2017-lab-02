# ML2017-lab-02
201530611142 陈安妮 5班   

## 逻辑回归、线性分类和随机梯度下降  

### 实验目的   

* 对比理解梯度下降和随机梯度下降的区别与联系。      
* 对比理解逻辑回归和线性分类的区别与联系。        
* 进一步理解SVM的原理并在较大数据上实践。        
### 数据集    
* 实验使用的是LIBSVM Data的中的a9a数据，包含32561 / 16281(testing)个样本，每个样本有123/123 (testing)个属性。    
### 实验步骤
  本次实验代码及画图均在jupyter上完成。      
#### 逻辑回归与随机梯度下降

* 1.读取实验训练集和验证集，使用sklearn库的load_svmlight_file函数读取数据。            
* 2.线性模型参数初始化，可以考虑全零初始化，随机初始化或者正态分布初始化。       
* 3.选择Loss函数及对其求导。      
* 4.求得部分样本对Loss函数的梯度G。       
* 5.使用不同的优化方法更新模型参数（NAG，RMSProp，AdaDelta和Adam）。     
* 6.选择合适的阈值，将验证集中计算结果大于阈值的标记为正类，反之为负类。在验证集上测试并得到不同优化方法的Loss函数值L{NAG}，L{RMSProp}，L{AdaDelta}和L{Adam}。      
* 7.重复步骤4-6若干次，画出L{NAG}，L{RMSProp}，L{AdaDelta}和L{Adam}随迭代次数的变化图。          

#### 线性分类与随机梯度下降

* 1.读取实验训练集和验证集，使用sklearn库的load_svmlight_file函数读取数据。            
* 2.线性模型参数初始化，可以考虑全零初始化，随机初始化或者正态分布初始化。       
* 3.选择Loss函数及对其求导。      
* 4.求得部分样本对Loss函数的梯度G。       
* 5.使用不同的优化方法更新模型参数（NAG，RMSProp，AdaDelta和Adam）。     
* 6.选择合适的阈值，将验证集中计算结果大于阈值的标记为正类，反之为负类。在验证集上测试并得到不同优化方法的Loss函数值L{NAG}，L{RMSProp}，L{AdaDelta}和L{Adam}。      
* 7.重复步骤4-6若干次，画出L{NAG}，L{RMSProp}，L{AdaDelta}和L{Adam}随迭代次数的变化图。          

整理实验结果并完成实验报告   
